[
  {
    "path": "posts/2021-10-15-graph-r-packages/",
    "title": "Probabilistic Graphical Modelling with bnlearn package",
    "description": {},
    "author": [
      {
        "name": "am-innocenter",
        "url": {}
      }
    ],
    "date": "2022-05-07",
    "categories": [
      "DAGs",
      "Bayesian networks",
      "R-packages"
    ],
    "contents": "\nAn analysis for R-Packages built for graphs\nDAGs are\n\n\n\n\n\n\n",
    "preview": "posts/2021-10-15-graph-r-packages/graph-r-packages_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-05-07T17:17:49+12:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-11-26-understanding-us-student-loan-debt/",
    "title": "Understanding US Student Loan Debt",
    "description": "Perform data visualization and determine the most preferred method of payment for student loans in the US.",
    "author": [],
    "date": "2019-11-26",
    "categories": [
      "TidyTuesday",
      "Data viz"
    ],
    "contents": "\nThis week the data was inspired from the Dignity\n& Debt project. This will help in understanding and spreading\nawareness around Student Loan debt. The data source is here\nThe objective for this is to perform data visualization and determine\nthe most preferred method of payment for student loans in the US. I will\nuse patchwork package to combine plots.\nLoading the data\n\nRows: 291\nColumns: 10\n$ agency_name        <chr> \"Account Control Technology, Inc.\", \"Alliâ€¦\n$ year               <int> 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 1â€¦\n$ quarter            <int> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,â€¦\n$ starting           <dbl> 5807704381, 3693337631, 2364391549, 70421â€¦\n$ added              <dbl> 1040570567, NA, NA, NA, NA, NA, 104094670â€¦\n$ total              <dbl> 122602642, 113326847, 83853003, 99643903,â€¦\n$ consolidation      <dbl> 20081894, 11533809, 7377703, 3401361, 894â€¦\n$ rehabilitation     <dbl> 90952573, 86967994, 64227391, 85960328, 5â€¦\n$ voluntary_payments <dbl> 5485506.9, 4885225.1, 3939866.1, 2508999.â€¦\n$ wage_garnishments  <dbl> 6082668, 9939819, 8308043, 7773214, 54738â€¦\n\nOriginal data file can be accessed through the weekly TidyTuesday\nGithub reporsitory in this link\nand data source is here\nRenaming columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nagency_name\n\n\nTotal.starting\n\n\nTotal.added\n\n\nTotal.repaid\n\n\nConServe\n\n\n109.05668\n\n\n8.255166\n\n\n3.3928101\n\n\nAccount Control Technology, Inc.Â \n\n\n91.20824\n\n\n8.254341\n\n\n2.8479518\n\n\nFMS Investment Corp\n\n\n73.20214\n\n\n8.254231\n\n\n2.1526409\n\n\nGC Services LP\n\n\n70.19962\n\n\n8.254341\n\n\n1.8947032\n\n\nWindham Professionals, Inc.Â \n\n\n65.77404\n\n\n8.254018\n\n\n2.0332805\n\n\nImmediate Credit Recovery, Inc.Â \n\n\n50.69030\n\n\n9.148463\n\n\n1.1516027\n\n\nImmediate Credit Recovery\n\n\n40.24292\n\n\n3.300990\n\n\n0.8600955\n\n\nFMS\n\n\n37.40026\n\n\n0.000000\n\n\n0.9890860\n\n\nCoast Professional, Inc.Â \n\n\n36.48558\n\n\n9.477421\n\n\n1.1540227\n\n\nGC Services\n\n\n35.37406\n\n\n0.000000\n\n\n0.9639185\n\n\nCoast Professional Inc\n\n\n35.36888\n\n\n7.604481\n\n\n0.9776974\n\n\nNational Recoveries Inc\n\n\n34.37731\n\n\n7.211969\n\n\n0.6891628\n\n\nAction Financial Services\n\n\n33.65348\n\n\n6.487008\n\n\n0.7403217\n\n\nNational Recoveries, Inc.Â \n\n\n31.86218\n\n\n8.850182\n\n\n0.7083161\n\n\n\n\n\n",
    "preview": "posts/2019-11-26-understanding-us-student-loan-debt/understanding-us-student-loan-debt_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-05-08T15:34:18+12:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-30-parkinjuries/",
    "title": "Amusement Park Injuries in TX, USA",
    "description": "Investigating how many injuries were recorded and the top causes of the injuries reported",
    "author": [],
    "date": "2019-09-19",
    "categories": [
      "TidyTuesday",
      "Data viz"
    ],
    "contents": "\nAmusement park injuries data is from data.world.\nThis is part of the weekly #TidyTuesday project aimed at the R ecosystem\non Twitter.\nThe data has a lot of text, inconsistent NAs and dates.\nObjectives\nClean the data\nPerform EDA and Data viz\nTo achieve the objectives we shall answer the following questions\nHow many injuries were recorded per year, per month?\nWhat kind of injuries were reported? What were the top causes of\ninjuries?\nWho were injured more? Children? Adults? Female/Girls?\nMale/Boys?\nLoading packages\n\n\n\n\n\n\n\nRows: 542\nColumns: 13\n$ injury_report_rec <dbl> 2032, 1897, 837, 99, 55, 780, 253, 253, 55â€¦\n$ name_of_operation <chr> \"Skygroup Investments LLC DBA iFly Austin\"â€¦\n$ city              <chr> \"Austin\", \"Galveston\", \"Grapevine\", \"San Aâ€¦\n$ st                <chr> \"TX\", \"TX\", \"TX\", \"TX\", \"AZ\", \"TX\", \"TX\", â€¦\n$ injury_date       <chr> \"2/12/2013\", \"3/2/2013\", \"3/3/2013\", \"3/3/â€¦\n$ ride_name         <chr> \"I Fly\", \"Gulf Glider\", \"Howlin Tornado\", â€¦\n$ serial_no         <chr> \"SV024\", \"GS-11-10-WG-14\", \"0643-C1-T1-TN6â€¦\n$ gender            <chr> \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"Mâ€¦\n$ age               <chr> \"37\", \"43\", NA, \"51\", \"17\", \"40\", \"36\", \"2â€¦\n$ body_part         <chr> \"Mouth\", \"Knee\", \"Right Shoulder\", \"Lower â€¦\n$ alleged_injury    <chr> \"Student hit mouth on wall\", \"Alleged arthâ€¦\n$ cause_of_injury   <chr> \"Student attempted unfamiliar manuever\", \"â€¦\n$ other             <chr> NA, \"Prior history of problems with this kâ€¦\n\n AZ  FL  TX \n  5   1 536 \n[1] \"F\" \"M\" NA  \"m\"\n\nData Cleaning\n\nWe shall drop data from the other states and only consider TX. Rename\nthe two factors of gender and drop NA Date and age is a character, we\nshall convert to date type and numeric respectively\n\n\n\n\nThe date has two types - excel numeric and mdy format. Using a\ntwo-step process; I converted the date using the janitor\npackage for the excel numeric and to dates from character using the\nlubridate package.\n\n\n\nData Viz\nHere we determine how many injuries were recorded per month and per\nyear. To analyse which months need extra attention because we expect\nmore injurries in the park during summer compared to winter.\n\n\n\nThe numer of injuries over the years have been decreasing\nsignificantly. This could be to a number of reasons. To speculate the\nleast we are getting better at control measures on the parks to avoid\ninjuries.\nTo answer how many injuries were recorded per month we utilized the\nmonth.abb built-in constant to rearrange months from Jan -\nDec\n\n\n\nAs we had pre-empted, June-Aug has the highest number of\ninjuries.\nWho got injured more? Male or female? children or adults?\n\n\n\nTrick I learned to pick the colors use\nggplot_build(plotname).\nChildren reported more injuries compared to adults. Females are\nslightly more by 5% compared to the males.\nAnalyse the cause of injuries and body parts affected below using\ntidytext.\n\n\n\nThe most injured parts are head and shoulder among children and\nteenagers. For the elderly > 50 years it is the neck and shoulder.\nMost adults will be in the park taking care of the younger ones - the\nmost injured body parts are shoulder and head.\n\n\n\nI wanted to determine the words that occur commonly in pairs and how\noften sequence of word1 and word 2 occurs\n\n\n\n#Learnings\n\nConverting excel numeric date format. Using tidytext, discovering\njanitor package,\n\nIn case you have feedback, questions, suggestions do not hesitate to\nleave a comment.\n\n\n\n",
    "preview": "posts/2020-03-30-parkinjuries/parkinjuries_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-05-08T15:37:49+12:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-30-eidobserved/",
    "title": "Data mining and wrangling : A case of Eid Occurance",
    "description": "Data mining with rvest; case study of Eid observations",
    "author": [],
    "date": "2019-06-05",
    "categories": [
      "Data Mining",
      "Data Viz"
    ],
    "contents": "\nFinally, Eid is here! Id-Ul-Fitr commonly known as Eid marks the end\nof fasting (Ramadhan) and is the first day of the Islamic month\nShawwal. I spent my weekend learning data mining with rvest and\nanalysing data for Eid observations.\nThe following link\ncontains information about Eid, its occurence of per Day, Month, year.\nFor today, I intend to only mine the table and explore.\nObjectives\nMine data - table - from a URL using the package rvest (this was\nmy first time and its really simple)\nExplore basic data cleaning using dplyr (this is included in\ntidyverse library)\nExplore the occurence of Eid i.e per month, per day\nPerform visualization using ggplot :-).\nThis is a learning curve and feel free to drop in your comments\nand/or suggestions. I will show you a bit of my thought process when\nanalysing and wrangling data.\nLetâ€™s go ðŸ˜„ ðŸ’ƒ\nSide note check this link and this to learn more\nabout including emojis in a markdown.\nLoading packages required\n\n\n\nData Mining\nLoading the data - but first we have to mine it from the URL\nprovided\n\n{xml_nodeset (3)}\n[1] <table class=\"tb-quick-facts\">\\n<tr>\\n<th>This year:<\/th>\\n<td> ...\n[2] <table id=\"tb-hol_obs\" class=\"tb-theme fw sep\">\\n<thead><tr>\\n< ...\n[3] <table class=\"tb-quick-facts\"><tbody>\\n<tr>\\n<th>English<\/th>\\n ...\n\nThere are 3 tables - no idea which one contains the Eid data. I will\nextract all the tables - out of curiosity and theyâ€™re only 3. If we had\nseveral tables - we could explicity use the table names e.g\nhtml_nodes(â€˜#table2â€™)\n\nList of 3\n $ : tibble [4 Ã— 2] (S3: tbl_df/tbl/data.frame)\n  ..$ X1: chr [1:4] \"This year:\" \"Next year:\" \"Last year:\" \"Type:\"\n  ..$ X2: chr [1:4] \"Mon, 2 May 2022 Tue, 3 May 2022\" \"Sun, 23 Apr 2023\" \"Fri, 14 May 2021\" \"Public holiday\"\n $ : tibble [13 Ã— 5] (S3: tbl_df/tbl/data.frame)\n  ..$ Year        : int [1:13] 2017 2018 2019 2020 2020 2021 2022 2022 2023 2024 ...\n  ..$ Weekday     : chr [1:13] \"Mon\" \"Sat\" \"Wed\" \"Sun\" ...\n  ..$ Date        : chr [1:13] \"26 Jun\" \"16 Jun\" \"5 Jun\" \"24 May\" ...\n  ..$ Name        : chr [1:13] \"Eid al-Fitr\" \"Eid al-Fitr\" \"Eid al-Fitr\" \"Eid al-Fitr\" ...\n  ..$ Holiday Type: chr [1:13] \"Public holiday\" \"Public holiday\" \"Public holiday\" \"Public holiday\" ...\n $ : tibble [4 Ã— 2] (S3: tbl_df/tbl/data.frame)\n  ..$ X1: chr [1:4] \"English\" \"German\" \"Norwegian\" \"Swahili\"\n  ..$ X2: chr [1:4] \"Eid al-Fitr, End of Ramadan\" \"Eid al-Fitr (Fest des Fastenbrechens)\" \"Eid al-Fitr, Slutt pÃ¥ ramadan\" \"Idd el Fitr, Mwisho wa Ramadhani\"\n\nThe second table contains information we are interested in - it has\nEid occurance data from the year 2015 - 2025 (some are predictions).\n\n\n\nAnother method to extact the table is by creating an empty list and\npopulating it with data\n\n\n\nEDA and cleaning\n\ntibble [13 Ã— 5] (S3: tbl_df/tbl/data.frame)\n $ Year        : int [1:13] 2017 2018 2019 2020 2020 2021 2022 2022 2023 2024 ...\n $ Weekday     : chr [1:13] \"Mon\" \"Sat\" \"Wed\" \"Sun\" ...\n $ Date        : chr [1:13] \"26 Jun\" \"16 Jun\" \"5 Jun\" \"24 May\" ...\n $ Name        : chr [1:13] \"Eid al-Fitr\" \"Eid al-Fitr\" \"Eid al-Fitr\" \"Eid al-Fitr\" ...\n $ Holiday Type: chr [1:13] \"Public holiday\" \"Public holiday\" \"Public holiday\" \"Public holiday\" ...\n\nThe data contains (13, 5) - that is 12 observations and 5\nvariables\nIn the year 2015 - there exists two entries the second one is the\nobserved and thus will delete the first entry\n\n\n\nFrom the structure above, we can see that the names of weekday is\nwritten in some language for example Sunday is J2 - I checked the\nEnglish equivalence in the website and replaced them.\nThe column names and data types are\n\n[1] \"Year\"         \"Weekday\"      \"Date\"         \"Name\"        \n[5] \"Holiday Type\"\n[1] \"integer\"   \"character\" \"character\"\n\nThe analysis will be based on the month and hence I separate the day\nfrom the month in column Date\n\n\n\nThe month of May is written as Mei - I replaced that and the weekday\nas shown below.\n\nFrom the website: Sun-J2, Mon-J3, Tue-J4, Wed-J5, Thu-Alh, Fri-Ij,\nSat-J1\n\n\n\n\nI had to do these one item after another - I will figure out a way\nnext time maybe a loop\nThe replace and gsub functions worked for just one\nitem - I tried concatinating the other and got an error while compiling\nor the matching was not exactly correct. I got this error with gsub\nargument â€˜replacementâ€™ has length > 1 and only the first element\nwill be used\nI also replaced the rest\nHere is our cleaned data\n\n# A tibble: 12 Ã— 6\n    Year Weekday Day   Month Name                    `Holiday Type`\n   <int> <chr>   <chr> <chr> <chr>                   <chr>         \n 1  2018 Sat     16    Jun   Eid al-Fitr             Public holiday\n 2  2019 Wed     5     Jun   Eid al-Fitr             Public holiday\n 3  2020 Sun     24    May   Eid al-Fitr             Public holiday\n 4  2020 Mon     25    May   Day off for Eid al-Fitr Public holiday\n 5  2021 Fri     14    May   Eid al-Fitr             Public holiday\n 6  2022 Mon     2     May   Eid al-Fitr             Public holiday\n 7  2022 Tue     3     May   Eid al-Fitr Holiday     Public holiday\n 8  2023 Sun     23    Apr   Eid al-Fitr             Public holiday\n 9  2024 Thu     11    Apr   Eid al-Fitr             Public holiday\n10  2025 Tue     1     Apr   Eid al-Fitr             Public holiday\n11  2026 Sat     21    Mar   Eid al-Fitr             Public holiday\n12  2027 Thu     11    Mar   Eid al-Fitr             Public holiday\n\n\n# A tibble: 4 Ã— 2\n  Month Month_occurence\n  <chr>           <int>\n1 Apr                 3\n2 Jun                 2\n3 Mar                 2\n4 May                 5\n\nFor the past 12 years each month has been represented three times except\nfor July.\n\n# A tibble: 7 Ã— 2\n  Weekday Weekday_occurence\n  <chr>               <int>\n1 Fri                     1\n2 Mon                     2\n3 Sat                     2\n4 Sun                     2\n5 Thu                     2\n6 Tue                     2\n7 Wed                     1\n\nEid was observed mostly on Monday from the year 2015 - 2025 - this is\nthe future ðŸ˜‰\nVisualization\nOur fourth objective was to visualize the data and get an insight on\nthe month or day that Eid is observed for the 12 years.\n\n\n\n\n\n\n\nTake home notes : this was a refresher for me and aluta continua\n\nI hope you have enjoyed this - though short ðŸ˜„\n\nFrom this data, the prediction is that next year - Eid will occur on\na Monday in the month of May - I shall sit tight and wait for it.\n\n\nHave a blessed Eid\n\n\n\n\n",
    "preview": "posts/2020-03-30-eidobserved/eidobserved_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2022-05-08T16:53:41+12:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
